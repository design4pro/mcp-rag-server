---
description: AI rules derived by SpecStory from the project AI interaction history
globs: *
---

## PROJECT RULES

- Documentation should be updated after every code change
- All code, comments, and technical documentation should be written in English
- The project follows modern libs-first architecture
- The user prefers using Zod for data validation and schema structure definition
- The project is implementing an MCP RAG server using Qdrant, mem0, and Gemini API, following a phased approach.
- When possible, prefer open-source solutions.
- Qdrant is running locally in Docker and does not require an API key.
- Use a self-hosted and open-source version of mem0. Open source mem0 does not need a key.
- The MCP RAG server health check must be verified to ensure Qdrant, Mem0, and RAG are active.
- Extra inputs are not permitted during config validation. Set `extra = "ignore"` in the Config class of Pydantic models.
- Set `extra = "ignore"` in the Config class of Pydantic models.
- Use basic memory notes for documentation, action plans, and task lists.
- Project documentation is located in `docs/` with phase-specific folders.
- All documentation should use Obsidian-style links `[[filename]]` or `[[folder/filename|Link Text]]`.
- Use basic memory `write_note` tool to generate documentation.
- Before creating new documentation, always ensure you are operating within the correct memory project for this project. If a memory project for this project does not exist, create it, switch to it, and set it as default.
- Documentation files should be structured in subfolders and named in kebab-case format.
- The AI assistant must always ensure it is operating within the correct memory project for this project before creating or updating documentation. If a dedicated memory project does not exist, it should create one and set it as the default.
- Documentation files must be structured in subfolders and named in kebab-case format. This ensures readability, Obsidian compatibility, and easy linking.
- When naming documentation files, use kebab-case (e.g., `installation-guide.md`, `api-reference.md`).
- If a documentation file contains multiple topics, break it down into smaller, atomic notes following Obsidian best practices.
- Internal links should use Obsidian-style links with the new folder structure and kebab-case filenames: `[[04-development/project-refactoring]]` or `[[../03-api/api-reference]]`.
- The documentation folder `docs/` should contain all subfolders and files.
- Documentation, action plans, and task lists must be prepared using basic memory notes.
- When preparing documentation, plan of actions, and lists of tasks, always use the documentation preparation workflow.
- During integration tests, `Mem0Service` must be initialized in a way that **bypasses any external API requirements** and uses only local file-based storage.
- All MCP RAG tools must function correctly and stably.
- The MCP RAG server should support configuration via environment variables to separate "memory" between projects. This includes the ability to change the Qdrant collection name or user ID.

## TECH STACK

- MCP (FastMCP)
- Qdrant (vector database)
- mem0 (memory layer for AI personalization)
- Gemini API (embeddings and text generation)
- qdrant-client
- mem0ai>=0.1.110
- google-genai
- Python (>=3.9)
- Pydantic
- Libraries for document chunking
- langchain>=0.1.0
- langchain-text-splitters>=0.0.1
- sentence-transformers>=2.2.0
- numpy>=1.24.0
- tiktoken>=0.5.0
- aiohttp
- fastapi
- uvicorn
- cv2 (OpenCV)
- librosa
- psutil

## PROJECT DOCUMENTATION & CONTEXT SYSTEM

- Project documentation is maintained using Obsidian-style markdown.
- The user prefers the Coursor IDE for `mcpServers` configuration.
- When configuring Coursor IDE, use a `mcpServers.json` file like this:

```json
[
  {
    "name": "MCP RAG Server",
    "description": "Local MCP RAG server with Qdrant (Docker), self-hosted mem0, Gemini API",
    "command": "python",
    "args": ["run_server_http.py"],
    "env": {
      "GEMINI_API_KEY": "your_gemini_api_key_here",
      "QDRANT_URL": "http://localhost:6333",
      "MEM0_SELF_HOSTED": "true",
      "MEM0_LOCAL_STORAGE_PATH": "./mem0_data",
      "FASTMCP_HOST": "127.0.0.1",
      "FASTMCP_PORT": "8001"
    }
  }
]
```

- Troubleshooting documentation is located in `docs/05-troubleshooting/troubleshooting-guide.md`.
- Project documentation is organized following Obsidian best practices.
- Each note should address a single concept (atomic note structure).
- Use Obsidian-style cross-references between related notes.
- Avoid content duplication across documentation.
- Maintain a clear organization and navigation.
- Ensure comprehensive coverage of all topics.
- The `docs/` directory is structured as follows:

```
docs/
├── 00-overview/                    # Metadata and indexes
├── 01-architecture/            # Core functionalities
├── 02-installation/            # System architecture
├── 03-api/                    # Development and testing
├── 04-development/             # Development and testing
├── 05-troubleshooting/              # Operations and monitoring
└── 99-templates/                  # Project phases
```

- Phase documentation is located in `docs/04-development/phases/` with phase-specific folders.
- All documentation should use Obsidian-style links `[[filename]]` or `[[folder/filename|Link Text]]`.
- File naming conventions:
  - `README.md`: Main overview for each phase
  - `implementation-plan.md`: Detailed implementation plans
  - `technical-specs.md`: Technical specifications (when needed)
  - `testing-guide.md`: Testing procedures (when needed)
- Documentation files should be structured in subfolders and named in kebab-case format.
  - Example:
```
docs/
  00-overview/
    project-overview.md
    documentation-index.md
  01-architecture/
    system-architecture.md
  02-installation/
    installation-guide.md
  03-api/
    api-reference.md
  04-development/
    development-phases.md
    project-refactoring.md
  05-troubleshooting/
    troubleshooting-guide.md
```

## CODING STANDARDS

- Configuration files should use Pydantic Settings with validation.
- Set `extra = "ignore"` in the Config class of Pydantic models.

## DEBUGGING

- After changes, always verify the MCP RAG server health check to ensure services (Qdrant, Mem0, Gemini, RAG) are correctly initialized and running.
- Use the `test_connections.py` script to check service connections.
- Enable debug logging by setting `LOG_LEVEL=DEBUG` when running the server.
- Check service logs for Docker-based services using `docker logs <container_id>`.
- Ensure Qdrant is running correctly. Use `curl http://localhost:6333/` to check that Qdrant is accessible, and confirm the response indicates the service is running.
- Ensure self-hosted mem0 works without needing an API key.
- During integration tests, `Mem0Service` must be initialized in a way that **bypasses any external API requirements** and uses only local file-based storage.
- If `get_memory_context` or `get_user_memories` from `Mem0Service` raises an error, ensure that the methods `get_relevant_memories` and `get_memories` are implemented in `Mem0Service`.
- If `list_documents` or `get_document_stats` from `QdrantService` raises an error, ensure the `list_documents` method is implemented in `QdrantService`.
- If `get_document` returns "Document not found," verify that the document ID exists in the Qdrant collection.
- If MCP RAG tools report errors related to missing attributes or methods in `QdrantService` or `Mem0Service`, first restart the MCP RAG server and verify the runtime environment.
- If encountering `'QdrantService' object has no attribute 'list_documents'` after deploying changes:
  - Ensure the server is running the correct version of the code.
  - Verify that the `list_documents` method exists in `src/mcp_rag_server/services/qdrant_service.py`.
  - Clean Python cache files (`find . -name "*.pyc" -delete && find . -name "__pycache__" -type d -exec rm -rf {} + 2>/dev/null || true`).
  - Restart the server.
  - Ensure the server is running with `PYTHONPATH` set to include the `src/` directory or install the project in editable mode (`pip install -e .`).
  - If using Docker, rebuild the Docker image to ensure the latest code is used.

## WORKFLOW & RELEASE RULES

- Always stage all changes using `git add .` before committing.
- Commit changes with descriptive messages, including details of reorganizations, new files, updates, and cleanup.
- Always check `git status` before committing to review changes.
- After committing, verify there are no remaining untracked files (excluding auto-generated files like SpecStory history files, unless specifically needed).
- Before moving to the next task, always check the current project status and action plan.
- The project will be developed in phases:
  - Phase 1: Foundations (High Priority)
    - [x] Initialize Python project with dependencies
    - [x] Implement basic MCP server
    - [x] Configure environment management
    - [x] Integrate Gemini API for embeddings
    - [x] Basic Qdrant service
  - Phase 2: RAG Core (High Priority)
    - [x] Document processing pipeline
    - [x] Embedding generation
    - [x] Vector storage and search system
    - [x] Basic search functionality
    - [x] RAG query pipeline
  - Phase 3: MCP Integration (Medium Priority) - **COMPLETED**
    - [x] Document management tools (comprehensive implementation)
    - [x] Search and query tools (comprehensive implementation)
    - [x] Memory management tools (comprehensive implementation)
    - [x] Data access resources (document and memory resources)
    - [x] Error handling and validation (Pydantic schemas)
    - [x] Complete MCP tool registration system
    - [x] Proper tool and resource organization
  - Phase 4: Memory Integration (Medium Priority) - **COMPLETED**
    - [x] mem0 service integration (basic setup)
    - [x] Memory storage infrastructure
    - [x] Basic memory CRUD operations
    - [x] Memory-aware RAG queries with semantic search (COMPLETED)
    - [x] User session management
    - [x] Advanced memory context retrieval
  - Phase 5: Advanced Features (Low Priority) - **COMPLETED**
    - [x] Document preprocessing and chunking improvements
    - [x] Advanced search filters
    - [x] Performance monitoring
    - [x] Complete documentation
    - [x] Tests and examples
    - [x] Fix broken MCP RAG tools
    - [x] Comprehensive testing and validation
  - Phase 6: Advanced AI Features (Medium Priority) - **COMPLETED**
    - [x] Advanced AI Reasoning
    - [x] Enhanced Memory Systems
  - Phase 6.1: Performance Optimization and Advanced Features (High Priority)
    - [ ] Performance Profiling and Analysis
    - [ ] Reasoning Engine Optimization
    - [ ] Context Service Optimization
    - [ ] Advanced Features Implementation
    - [ ] Memory System Optimization
- Add option to Docker startup script to rebuild the image for the most up-to-date code.

## DOCUMENTATION PREPARATION WORKFLOW

### Overview

This document outlines the standardized workflow for preparing project documentation using Basic Memory tools, following the successful restructuring of the MCP RAG Server documentation.

### Workflow Steps

#### 1. Project Setup and Memory Management

##### Before Starting Documentation Work
- **Verify Memory Project**: Always ensure you're on the correct memory project for the current project
- **Set Default Project**: If needed, create and set the memory project as default
- **Check Project Status**: Use `get_current_project()` to confirm active project

##### Memory Project Commands
```bash
# List available projects
mcp_c-basic-memory_list_memory_projects()

# Switch to project
mcp_c-basic-memory_switch_project("project-name")

# Set as default
mcp_c-basic-memory_set_default_project("project-name")

# Create new project if needed
mcp_c-basic-memory_create_memory_project("project-name", "project-path", set_default=True)
```

#### 2. Documentation Structure Planning

##### Folder Organization
Always organize documentation in logical subfolders:
```
docs/
├── 00-overview/           # Main documentation index and project overview
├── 01-architecture/       # System architecture and design
├── 02-installation/       # Setup and installation guides
├── 03-api/               # API documentation and references
├── 04-development/       # Development guides, phases, refactoring
└── 05-troubleshooting/   # Troubleshooting and debugging guides
```

##### Naming Conventions
- **Kebab-case**: All file names must use kebab-case format
  - ✅ `installation-guide.md`
  - ✅ `api-reference.md`
  - ✅ `troubleshooting-guide.md`
  - ❌ `installation_guide.md`
  - ❌ `InstallationGuide.md`

#### 3. Content Generation Process

##### Using Basic Memory write_note Tool
Always use `mcp_c-basic-memory_write_note` for creating documentation:

```python
mcp_c-basic-memory_write_note(
    title="filename-without-extension",
    content="# Document Title\n\nContent here...",
    folder="docs/subfolder-name",
    tags=['relevant', 'tags', 'obsidian-compatible']
)
```

##### Content Requirements
- **Atomic Notes**: Each note should address a single concept
- **Obsidian Links**: Use proper Obsidian-style linking
  - `[[../folder/filename]]` for relative links
  - `[[filename|display text]]` for custom display text
- **English Language**: All content must be in English
- **Cross-references**: Include related documentation links

#### 4. Link Management

##### Obsidian-Style Linking
- **Relative Paths**: Use `../` for parent directory navigation
- **Consistent Format**: Always use `[[../folder/filename]]` pattern
- **Display Text**: Use `[[../folder/filename|Display Text]]` for better readability

##### Example Link Structure
```markdown
## Related Documentation

- [[../01-architecture/system-architecture|System Architecture]]
- [[../02-installation/installation-guide|Installation Guide]]
- [[../03-api/api-reference|API Reference]]
- [[../04-development/development-phases|Development Phases]]
- [[../05-troubleshooting/troubleshooting-guide|Troubleshooting Guide]]
```

#### 5. Quality Assurance

##### Before Finalizing
- **Check All Links**: Ensure all cross-references are correct
- **Verify Structure**: Confirm folder organization is logical
- **Test Obsidian Compatibility**: Verify links work in Obsidian
- **Review Content**: Ensure atomic note structure is maintained

##### Validation Checklist
- [ ] All files use kebab-case naming
- [ ] All links use proper Obsidian format
- [ ] Each note addresses single concept (atomic)
- [ ] Cross-references are comprehensive
- [ ] Content is in English
- [ ] Tags are relevant and descriptive

#### 6. Project Integration

##### Update Main README
Always update the main project README.md with new documentation links:

```markdown
## Documentation

Comprehensive documentation is available in the `docs/` directory:

- **Documentation Index**: [[docs/00-overview/documentation-index.md]]
- **Project Overview**: [[docs/00-overview/project-overview.md]]
- **System Architecture**: [[docs/01-architecture/system-architecture.md]]
- **Installation Guide**: [[docs/02-installation/installation-guide.md]]
- **API Reference**: [[docs/03-api/api-reference.md]]
- **Development Phases**: [[docs/04-development/development-phases.md]]
- **Project Refactoring**: [[docs/04-development/project-refactoring.md]]
- **Troubleshooting**: [[docs/05-troubleshooting/troubleshooting-guide.md]]
```

### Best Practices

#### Documentation Standards
1. **Consistency**: Follow established patterns and conventions
2. **Completeness**: Ensure comprehensive coverage of topics
3. **Clarity**: Write clear, concise, and well-organized content
4. **Maintainability**: Structure for easy updates and modifications

#### Technical Requirements
1. **Basic Memory Integration**: Always use `write_note` for creation
2. **Obsidian Compatibility**: Ensure proper markdown and linking
3. **Version Control**: Commit documentation with related code changes
4. **Cross-Platform**: Ensure compatibility across different systems

#### Workflow Efficiency
1. **Plan First**: Always create a plan before starting
2. **Batch Operations**: Group related documentation tasks
3. **Validation**: Verify each step before proceeding
4. **Documentation**: Document the documentation process itself

### Example Workflow

#### For New Project Documentation
1. **Setup**: Verify/create memory project
2. **Plan**: Create documentation structure plan
3. **Create**: Generate atomic notes using `write_note`
4. **Link**: Establish cross-references
5. **Validate**: Check all links and structure
6. **Integrate**: Update main README and project files

#### For Documentation Updates
1. **Backup**: Create backup of existing documentation
2. **Plan**: Identify what needs to be updated
3. **Execute**: Apply changes using `write_note`
4. **Test**: Verify all links and references
5. **Commit**: Update version control with changes

### Success Metrics

#### Quality Indicators
- ✅ All files follow kebab-case naming
- ✅ All links use proper Obsidian format
- ✅ Atomic note structure maintained
- ✅ Comprehensive cross-referencing
- ✅ English language throughout
- ✅ Logical folder organization

#### Efficiency Indicators
- ✅ Documentation created using Basic Memory tools
- ✅ Consistent workflow followed
- ✅ All validation steps completed
- ✅ Project integration successful

## Related Documentation

- [[../00-overview/documentation-index|Documentation Index]]
- [[../04-development/project-refactoring|Project Refactoring History]]
- [[../02-installation/installation-guide|Installation Guide]]

## GIT Workflow Rules

- Always stage all changes using `git add .` before committing.
- Commit changes with descriptive messages, including details of reorganizations, new files, updates, and cleanup.
- Always check `git status` before committing to review changes.
- After committing, verify there are no remaining untracked files (excluding auto-generated files like SpecStory history files, unless specifically needed).
- Before moving to the next task, always check the current project status and action plan.

## Phase 5 Completion and Phase 6 Planning

### Overview

#### Phase 5
- [x] Fix broken MCP RAG tools
- [x] Comprehensive testing and validation
- [x] Complete documentation

#### Phase 6
- [x] Advanced AI Reasoning
- [x] Enhanced Memory Systems

## MCP RAG Tool Testing and Error Summary

### Overview

This section summarizes the results of testing all available MCP RAG tools, identifying tools that are functioning correctly and those with errors.

### Functioning Tools ✅

#### 1. Health Check
- `mcp_rag_health_check` - ✅ Works
  - All services (Gemini, Qdrant, Mem0, Session, RAG) are active.

#### 2. Document Management
- `mcp_rag_add_document` - ✅ Works
- `mcp_rag_delete_document` - ✅ Works

#### 3. Search and Query
- `mcp_rag_search_documents` - ✅ Works
- `mcp_rag_batch_search` - ✅ Works
- `mcp_rag_get_search_suggestions` - ✅ Works

#### 4. Memory Management
- `mcp_rag_add_memory` - ✅ Works
- `mcp_rag_delete_memory` - ✅ Works

#### 5. Session Management
- `mcp_rag_create_session` - ✅ Works
- `mcp_rag_get_session_info` - ✅ Works
- `mcp_rag_list_user_sessions` - ✅ Works
- `mcp_rag_get_session_stats` - ✅ Works
- `mcp_rag_expire_session` - ✅ Works

#### 6. System Statistics
- `mcp_rag_get_system_session_stats` - ✅ Works

#### 7. RAG Question Answering
- `mcp_rag_ask_question` - ✅ Works (but requires documents in context)

### Tools with Errors ❌

#### 1. Document Management
- `mcp_rag_list_documents` - ❌ Error: `'QdrantService' object has no attribute 'list_documents'`
- `mcp_rag_get_document_stats` - ❌ Error: `'QdrantService' object has no attribute 'list_documents'`
- `mcp_rag_get_document` - ❌ Error: "Document not found"

#### 2. Memory Management
- `mcp_rag_get_user_memories` - ❌ Error: `'Mem0Service' object has no attribute 'get_memories'`
- `mcp_rag_get_memory_context` - ❌ Error: `'Mem0Service' object has no attribute 'get_relevant_memories'`
- `mcp_rag_clear_user_memories` - ❌ Error: `'Mem0Service' object has no attribute 'clear_memories'`
- `mcp_rag_get_user_session_info` - ❌ Error: `'Mem0Service' object has no attribute 'get_memories'`

### Test Statistics 📊

- **Total tools tested:** 20
- **Functioning correctly:** 13 (65%)
- **With errors:** 7 (35%)

### Main Issues to Fix 🔧

1.  **QdrantService** - missing implementation of `list_documents` method.
2.  **Mem0Service** - missing implementation of the following methods:
    -   `get_memories`
    -   `get_relevant_memories`
    -   `clear_memories`

### Recommendations 💡

1.  **Priority 1:** Fix missing methods in `QdrantService` and `Mem0Service`.
2.  **Priority 2:** Improve error handling in `get_document` (document exists but is not found).
3.  **Priority 3:** Add better context handling in `ask_question`.

### Docker Setup (Alternative Installation)

#### Prerequisites
- **Docker**: 20.10 or higher
- **Docker Compose**: 2.0 or higher

#### Quick Start with Docker
```bash
# Clone repository
git clone <repository-url>
cd rag

# Copy environment template
cp .env.example .env

# Edit .env with your API keys
nano .env

# Start all services with Docker
./scripts/manage_docker.sh start
```

**Note**: Health checks have been optimized for faster startup:
- Qdrant: 10s intervals, 5s timeout, 15s start period
- MCP Server: 30s intervals, 10s timeout, 3 retries

#### Docker Management Commands
```bash
# Start services
./scripts/manage_docker.sh start

# Stop services
./scripts/manage_docker.sh stop

# Restart services
./scripts/manage_docker.sh restart

# Show status
./scripts/manage_docker.sh status

# View logs
./scripts/manage_docker.sh logs

# Build Docker image (with cache)
./scripts/manage_docker.sh build

# Rebuild Docker image from scratch (no cache)
./scripts/manage_docker.sh rebuild

# Clean up Docker resources
./scripts/manage_docker.sh cleanup

# Show environment info
./scripts/manage_docker.sh env

# Show volume information and data
./scripts/manage_docker.sh volumes
```

#### Docker Rebuild Options
When you make changes to the code and want to update the Docker image:

**Option 1: Quick rebuild (with cache)**
```bash
./scripts/manage_docker.sh build
./scripts/manage_docker.sh restart
```

**Option 2: Fresh rebuild (no cache) - recommended for major changes**
```bash
./scripts/manage_docker.sh rebuild
```
This will:
- Stop running services
- Remove existing image
- Build completely fresh image
- Optionally start services

#### Data Persistence During Rebuild
**Important**: Your data is preserved during rebuild operations!

The following data is stored in Docker volumes and persists across rebuilds:
- **Mem0 memories**: Stored in `mem0_data` volume (`memories.json`)
- **Qdrant documents**: Stored in `qdrant_data` volume (collections, embeddings)
- **User sessions**: Preserved in persistent storage

**What gets removed during rebuild:**
- Docker image layers (code changes)
- Container state (temporary data)
- Build cache

**What stays preserved:**
- All user memories and conversations
- All uploaded documents and embeddings
- All user sessions and metadata
- Database collections and indexes

To completely clear all data, use the cleanup command:
```bash
./scripts/manage_docker.sh cleanup
# Then manually remove volumes if needed:
# docker volume rm mcp-rag_mem0_data mcp-rag_qdrant_data
```

#### Checking Data Status
To verify what data is stored in your volumes:
```bash
./scripts/manage_docker.sh volumes
```

This command shows:
- List of all project volumes
- File and directory counts in each volume
- Specific files like `memories.json` for Mem0
- Qdrant collections status
- Tips for data management

Example output:
```
📁 mcp-rag_mem0_data
   Contains: 1 files, 0 directories
   📄 memories.json exists

📁 mcp-rag_qdrant_data
   Contains: 172 files, 54 directories
   🗄️  Qdrant collections exist
```

### Troubleshooting

#### 1. Docker Issues
**Problem**: Docker containers not starting or failing

**Solution**: Check Docker setup and rebuild if needed:
```bash
# Check Docker status
./scripts/manage_docker.sh status

# View detailed logs
./scripts/manage_docker.sh logs

# Rebuild from scratch if needed
./scripts/manage_docker.sh rebuild
```

**Problem**: Docker image outdated after code changes

**Solution**: Rebuild the Docker image:
```bash
# For minor changes (with cache)
./scripts/manage_docker.sh build

# For major changes (no cache)
./scripts/manage_docker.sh rebuild
```

**Problem**: Port conflicts with Docker

**Solution**: Check and change ports in docker-compose.yml:
```yaml
ports:
  - "8001:8001"  # Change 8001 to available port
```

#### 5. Qdrant "Waiting" Status

**Problem:** When starting the server with `start`, the Qdrant container continuously shows a "waiting" status, even though it initializes quickly.

**Solution:** This often indicates a health check issue. Verify the Qdrant service's health check configuration in `docker-compose.yml` and ensure it aligns with how Qdrant reports its health. The health check might be timing out prematurely. The health check has been optimized with:
- Shorter intervals (10s instead of 30s)
- Faster timeouts (5s instead of 10s)
- Proper start period (15s)

If you still see "waiting" status, check:
```bash
# Check container logs
./scripts/manage_docker.sh logs-service qdrant

# Check health check manually
docker exec mcp-rag-qdrant timeout 3 bash -c '</dev/tcp/localhost/6333' && echo "OK" || echo "FAILED"
```
### Manage Docker Script Usage

- To rebuild the Docker image from scratch, use the following command:
  ```bash
  ./scripts/manage_docker.sh rebuild
  ```
  This command stops the running services (if active), removes the existing Docker image, rebuilds the image with the `--no-cache` flag, and prompts the user to start the services after the rebuild.

### Differences Between Build and Rebuild Commands

- **`build`**: Performs a quick rebuild using cached layers (suitable for minor changes).
- **`rebuild`**: Performs a full rebuild without using cached layers (recommended for major changes).

Now you have full control over the Docker image building process, ensuring you always have the latest code in your containers.

## Sequential Task Analysis Capabilities in MCP RAG Server

### Overview

The MCP RAG Server provides comprehensive tools for sequential task analysis before execution. These capabilities enable systematic breakdown and understanding of complex tasks through multiple reasoning approaches.

### Available Analysis Tools

#### 1. Advanced Query Understanding
**Tool**: `mcp_rag_advanced_query_understanding`

**Purpose**: Deep semantic analysis of task queries
- Identifies entities and relationships
- Determines reasoning type and complexity
- Extracts semantic features and roles
- Provides confidence scoring

**Use Case**: Initial task decomposition and understanding

#### 2. Chain-of-Thought Reasoning
**Tool**: `mcp_rag_chain_of_thought_reasoning`

**Purpose**: Step-by-step logical reasoning
- Breaks down complex tasks into sequential steps
- Provides deductive reasoning at each step
- Maintains reasoning chain with confidence levels
- Configurable number of reasoning steps

**Use Case**: Detailed task analysis with logical progression

#### 3. Multi-Hop Reasoning
**Tool**: `mcp_rag_multi_hop_reasoning`

**Purpose**: Multi-stage reasoning with context building
- Performs iterative reasoning across multiple hops
- Builds context progressively
- Maintains plan validity throughout hops
- Provides comprehensive final context

**Use Case**: Complex task planning with iterative refinement

#### 4. Context Analysis
**Tool**: `mcp_rag_analyze_context`

**Purpose**: Contextual understanding of tasks
- Analyzes task context and requirements
- Identifies relevant background information
- Provides contextual insights for task execution

**Use Case**: Understanding task environment and constraints

#### 5. Memory-Enhanced Analysis
**Tool**: `mcp_rag_get_enhanced_memory_context`

**Purpose**: Leverages user memory for task analysis
- Incorporates historical context and patterns
- Provides personalized task understanding
- Enhances analysis with user-specific insights

**Use Case**: Personalized task analysis based on user history

### Sequential Analysis Workflow

#### Phase 1: Task Understanding
1. **Query Analysis**: Use `advanced_query_understanding` to decompose the task
2. **Context Extraction**: Analyze relevant context using `analyze_context`
3. **Memory Integration**: Incorporate user memory context

#### Phase 2: Task Planning
1. **Chain-of-Thought**: Break down task into logical steps
2. **Multi-Hop Reasoning**: Iteratively refine the plan
3. **Context Validation**: Ensure plan aligns with available context

#### Phase 3: Execution Preparation
1. **Resource Identification**: Determine required resources
2. **Dependency Mapping**: Identify task dependencies
3. **Risk Assessment**: Evaluate potential challenges

### Example Analysis Sequence

#### Input Task
"Przygotuj plan implementacji systemu zarządzania zadaniami z priorytetami, deadline'ami i przypisaniem użytkowników"

#### Analysis Results

##### 1. Query Understanding
- **Reasoning Type**: Planning
- **Complexity**: Simple
- **Confidence**: 0.8
- **Intent**: General planning task

##### 2. Chain-of-Thought Analysis
- **Steps Completed**: 3
- **Reasoning Type**: Deductive
- **Overall Confidence**: 0.5

##### 3. Multi-Hop Planning
- **Hops**: 1 completed
- **Plan Generated**: 4-step implementation plan
- **Plan Validity**: Confirmed

### Benefits of Sequential Analysis

#### 1. Systematic Approach
- Ensures comprehensive task understanding
- Prevents overlooking critical aspects
- Provides structured execution path

#### 2. Context Awareness
- Incorporates historical context
- Considers user-specific patterns
- Adapts to current environment

#### 3. Quality Assurance
- Multiple validation layers
- Confidence scoring at each step
- Iterative refinement capabilities

#### 4. Resource Optimization
- Identifies required resources early
- Prevents resource conflicts
- Optimizes execution efficiency

### Integration with RAG Capabilities

#### Document Context
- Leverages stored documents for task analysis
- Provides relevant examples and templates
- Enhances analysis with domain knowledge

#### Memory Integration
- Uses user memory for personalized analysis
- Incorporates historical task patterns
- Provides context-aware recommendations

#### Session Management
- Maintains analysis context across sessions
- Tracks analysis progress and results
- Enables collaborative task analysis

### Best Practices

#### 1. Analysis Depth
- Use appropriate analysis depth for task complexity
- Balance thoroughness with efficiency
- Adapt analysis approach to task type

#### 2. Context Utilization
- Maximize use of available context
- Incorporate relevant documents and memories
- Consider environmental factors

#### 3. Validation
- Validate analysis results at each step
- Cross-reference with multiple reasoning approaches
- Ensure plan feasibility and completeness

#### 4. Documentation
- Document analysis process and results
- Maintain analysis history for future reference
- Share insights across team members

### Technical Implementation

#### Service Architecture
- **Reasoning Service**: Core reasoning engine
- **Context Service**: Context management and analysis
- **Memory Service**: User memory integration
- **Session Service**: Analysis session management

#### Performance Considerations
- Optimized reasoning algorithms
- Efficient context retrieval
- Scalable memory management
- Fast response times for analysis requests

### Future Enhancements

#### Planned Features
- **Collaborative Analysis**: Multi-user task analysis
- **Advanced Reasoning**: More sophisticated reasoning algorithms
- **Visual Analysis**: Graphical task breakdown
- **Predictive Analysis**: Outcome prediction based on analysis

#### Integration Opportunities
- **Project Management Tools**: Direct integration with PM systems
- **Development Environments**: IDE integration for code tasks
- **Communication Platforms**: Team collaboration integration
- **Analytics Platforms**: Analysis performance tracking

### Conclusion

The MCP RAG Server's sequential task analysis capabilities provide a powerful foundation for systematic task understanding and planning. By combining multiple reasoning approaches with context awareness and memory integration, the system enables comprehensive task analysis before execution, leading to better outcomes and more efficient resource utilization.

## Project Isolation Configuration in MCP RAG Server

### Overview

The MCP RAG Server now supports project isolation to prevent data mixing between different projects, clients, and technologies. This feature ensures that each project maintains its own separate memory and document storage.

### Why Project Isolation is Important

#### Problems with Mixed Data
1. **Context Pollution**: Analysis may be based on irrelevant data from other projects
2. **False Associations**: System may connect concepts from different domains
3. **Reduced Precision**: Noise in data decreases response quality
4. **Privacy Issues**: Data from different clients gets mixed together
5. **Confusion in Analysis**: Sequential reasoning may use wrong context

#### Benefits of Project Isolation
1. **Clean Context**: Each project has its own isolated data space
2. **Better Accuracy**: Analysis is based only on relevant project data
3. **Privacy Protection**: Client data remains separate
4. **Focused Reasoning**: Sequential analysis uses project-specific context
5. **Eas